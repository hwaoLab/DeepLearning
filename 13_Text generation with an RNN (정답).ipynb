{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"text generation with an RNN (정답).ipynb","provenance":[{"file_id":"1oFNGv8rpK4rTQerlk75AFtRMNdpzKMiE","timestamp":1595421640718},{"file_id":"14QMN8BstnVcnXcDPYbbvMleBCRPfRijT","timestamp":1595415273791}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"qV40U7mLZkN4","colab_type":"text"},"source":["# Tensorflow 실습 : RNN을 이용한 텍스트 생성"]},{"cell_type":"markdown","metadata":{"id":"jcJihGy4bqCW","colab_type":"text"},"source":["- 학습 속도를 위해, GPU를 사용하기를 권장"]},{"cell_type":"code","metadata":{"id":"KAgXBP8LtArB","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","\n","import numpy as np\n","import os\n","import time\n","import matplotlib.pyplot as plt\n","\n","# history를 그래프로 그리기 위해 사용\n","def plot_graphs(history, metric):\n","  plt.plot(history.history[metric])\n","  plt.plot(history.history['val_'+metric], '')\n","  plt.xlabel(\"Epochs\")\n","  plt.ylabel(metric)\n","  plt.legend([metric, 'val_'+metric])\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jx6UkuneSmBU","colab_type":"text"},"source":["## Shakespeare's writing 데이터셋 "]},{"cell_type":"markdown","metadata":{"id":"h_QaFCx_juuI","colab_type":"text"},"source":["- 이 데이터셋은 Shakespeare의 writing을 모아놓은 데이터셋\n","- 본 실습에서는 해당 데이터셋을 이용하여, character level language model을 학습하고 이를 텍스트 생성에 활용하고자 함\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"EHDoRoc5PKWz"},"source":["### Download the Shakespeare dataset"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"pD_55cOxLkAb","colab":{}},"source":["path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"UHjdCjDuSvX_"},"source":["### Read the data\n","\n","먼저 다운로드한 파일을 살펴보면, "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"aavnuByVymwK","colab":{}},"source":["# Read, then decode for py2 compat.\n","text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n","# length of text is the number of characters in it\n","print ('Length of text: {} characters'.format(len(text)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Duhg9NrUymwO","colab":{}},"source":["# Take a look at the first 250 characters in text\n","print(text[:250])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"IlCgQBRVymwR","colab":{}},"source":["# The unique characters in the file\n","vocab = sorted(set(text))\n","print ('{} unique characters'.format(len(vocab)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"LFjSVAlWzf-N"},"source":["### Vectorize the text\n","\n","- 각각의 character를 정수(integer)의 index로 표현하기 위해, 두개의 look-up table을 정의함\n","  - character => 숫자\n","  - 숫자 => character"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"IalZLbvOzf-F","colab":{}},"source":["# 고유 문자에서 인덱스로 매핑 생성\n","char2idx = {u:i for i, u in enumerate(vocab)}\n","idx2char = np.array(vocab)\n","\n","text_as_int = np.array([char2idx[c] for c in text])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"tZfqhkYCymwX"},"source":["- 이제 각 character와 숫자를 encode, decode할 수 있는 look-up table이 준비되었음"]},{"cell_type":"code","metadata":{"id":"rY1sI31HmvJu","colab_type":"code","colab":{}},"source":["char2idx"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"l1VKcQHcymwb","colab":{}},"source":["# 텍스트에서 처음 13개의 문자가 숫자로 어떻게 매핑되었는지를 보여줍니다\n","print ('{} ---- 문자들이 다음의 정수로 매핑되었습니다 ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"bbmsf23Bymwe"},"source":["### language model 학습을 위한 input과 target 만들기\n","- language model을 학습하기 위해서는 첫번째 부터 t번째 step의 input sequence를 알고 있을 때, t+1번째 step에는 어떤 token이 나오는지에 대한 정답이 필요\n","- 이를 위해, 먼저 전체 텍스트를 일정한 character 개수 간격으로 나누어서 input sequence를 구성\n","- 각 input sequence에서 target을 만드는 방법은 input sequence에서 오른쪽으로 한칸씩 이동시키는 것\n","  - ex) `Hello` => input : `Hell` , target : `ello`\n"]},{"cell_type":"code","metadata":{"id":"sWF6GMrUqFlZ","colab_type":"code","colab":{}},"source":["# 단일 입력에 대해 원하는 문장의 최대 길이\n","seq_length = 100\n","examples_per_epoch = len(text)//seq_length\n","\n","# 훈련 샘플/타깃 만들기\n","char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n","\n","for i in char_dataset.take(5):\n","  print(idx2char[i.numpy()])\n","\n","print('텍스트 총 글자수 : ', len(text_as_int))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qu4k1Cv3qQy5","colab_type":"code","colab":{}},"source":["# batch를 이용하여 앞에서부터 순서대로, 총 101개의 문자를 하나의 문장으로 만들어 줌\n","# drop_remainder : 마지막 배치는 101개보다 작은 수로 문장이 만들어지므로 제거함\n","sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n","\n","for item in sequences.take(5):\n","  print(repr(''.join(idx2char[item.numpy()])))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FYCiiyCJq7tN","colab_type":"text"},"source":["- 각각의 101개의 글자로 이루어진 문장을 길이 100의 input sequence와 길이 100의 target sequence로 변환"]},{"cell_type":"code","metadata":{"id":"-u1CsO5SrHIJ","colab_type":"code","colab":{}},"source":["def split_input_target(chunk):\n","    input_text = chunk[:-1]\n","    target_text = chunk[1:]\n","    return input_text, target_text\n","\n","dataset = sequences.map(split_input_target)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TBGewfxFrL87","colab_type":"code","colab":{}},"source":["# input과 target 예시\n","for input_example, target_example in  dataset.take(1):\n","  print ('입력 데이터: ', repr(''.join(idx2char[input_example.numpy()])))\n","  print ('타깃 데이터: ', repr(''.join(idx2char[target_example.numpy()])))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xaV_LU4frggf","colab_type":"text"},"source":["### 모델 학습을 위해, mini batch로 구성\n"]},{"cell_type":"markdown","metadata":{"id":"nL5UR4aAsC65","colab_type":"text"},"source":["- 이전에 사용한 batch는 문장으로 나누기 위해 사용한 것이므로, 실제 mini batch로 나누는 과정을 적용\n","- 실험 과정을 간단히 하기 위해, 학습 데이터셋과 validation 데이터셋으로만 분리"]},{"cell_type":"code","metadata":{"id":"pljHigjhsAux","colab_type":"code","colab":{}},"source":["BATCH_SIZE = 64\n","BUFFER_SIZE = 10000\n","\n","train_dataset = dataset.take(examples_per_epoch // 4 * 3)\n","valid_dataset = dataset.skip(examples_per_epoch // 4 * 3)\n","\n","train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n","valid_dataset = valid_dataset.batch(BATCH_SIZE)\n","\n","print(train_dataset)\n","print(len(list(train_dataset)))\n","print(len(list(valid_dataset)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DNGDjZ5stz37","colab_type":"text"},"source":["## RNN 모델을 이용한 텍스트 생성"]},{"cell_type":"markdown","metadata":{"id":"K3lBBFyMTblr","colab_type":"text"},"source":["### RNN 모델\n","- embedding layer, LSTM, Dense layer로 구성\n","  - input에 padding이 없으므로, masking하지 않음\n","  - 모든 time step에 대해, 다음 time step의 target을 맞추도록 학습하기 위해 return_sequence=True\n","  - 마지막 dense layer는 다음 time step의 글자가 무엇인지 맞추어야 하므로, vocab size에 대해 logit 값을 계산"]},{"cell_type":"code","metadata":{"id":"DtBVc8MzTayj","colab_type":"code","colab":{}},"source":["# 문자로 된 어휘 사전의 크기\n","vocab_size = len(vocab)\n","\n","# 임베딩 차원\n","embedding_dim = 128\n","\n","# RNN 유닛(unit) 개수\n","rnn_units = 128\n","\n","model = tf.keras.Sequential([\n","    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim),\n","    tf.keras.layers.GRU(rnn_units, return_sequences=True),\n","    tf.keras.layers.Dense(vocab_size)\n","])\n","\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iT89E-R0vo5n","colab_type":"text"},"source":["![모델을 통과하는 데이터의 사진](https://tensorflow.org/tutorials/text/images/text_generation_training.png)"]},{"cell_type":"markdown","metadata":{"id":"hA9BpHdbv9Qc","colab_type":"text"},"source":["### 학습하기 전 모델 사용"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"C-_70kKAPrPU","colab":{}},"source":["for input_example_batch, target_example_batch in train_dataset.take(1):\n","  example_batch_predictions = model(input_example_batch)\n","  print(example_batch_predictions.shape, \"# (배치 크기, 시퀀스 길이, 어휘 사전 크기)\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sUirbA-fT2Mi","colab_type":"text"},"source":["### 옵티마이저(optimizer), 손실 함수, 평가 metric 선택"]},{"cell_type":"code","metadata":{"id":"QOB_5E7vUChh","colab_type":"code","colab":{}},"source":["# multi-label classification과 동일하므로, cross entropy loss 이용\n","# 마지막 dense layer에 sigmoid activation이 없으므로 from_logits=True\n","model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","              optimizer=tf.keras.optimizers.Adam(),\n","              metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C0Bbjb6UbMn6","colab_type":"text"},"source":["###모델 학습 \n","- 여기서는 model.fit을 이용하면서, 매번 epoch이 끝날때마다 체크포인트(모델의 파라마터)를 저장하는 방법을 다뤄보려고 함\n","  - tf.keras.callbacks.ModelCheckpoint"]},{"cell_type":"code","metadata":{"id":"A5OK_bjQyxle","colab_type":"code","colab":{}},"source":["# 체크포인트가 저장될 디렉토리\n","checkpoint_dir = './training_checkpoints'\n","\n","# 체크포인트 파일 이름\n","checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n","\n","# model.fit 메소드에 함께 이용할 callback \n","# 'val_loss'를 monitor하면서, 가장 val_loss가 작을때만 모델을 저장함 (save_best_only=True)\n","checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_prefix, \n","    save_weights_only=True, \n","    monitor='val_loss',\n","    save_best_only=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IMDUss0KbQFy","colab_type":"code","colab":{}},"source":["history = model.fit(train_dataset, epochs=20,\n","                    validation_data=valid_dataset,\n","                    callbacks=[checkpoint_callback])\n","                    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IjXM_olonKvZ","colab_type":"code","colab":{}},"source":["# 그래프 그리기\n","plot_graphs(history, 'accuracy')\n","plot_graphs(history, 'loss')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9duT_TSAbZIR","colab_type":"text"},"source":["### Best model 복원"]},{"cell_type":"markdown","metadata":{"id":"wrI-II8l3RTz","colab_type":"text"},"source":["- validation loss를 기준으로 best 모델만 저장했으므로, 마지막에 저장된 모델이 최종 Best 모델"]},{"cell_type":"code","metadata":{"id":"5R8mCWCb3kxk","colab_type":"code","colab":{}},"source":["print(tf.train.latest_checkpoint(checkpoint_dir))\n","model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M--byf6EdU7R","colab_type":"text"},"source":["### 학습된 모델 활용"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"DjGz1tDkzf-u"},"source":["학습된 모델을 이용하여 텍스트를 생성하려고 함\n","\n","* 시작 문자열 선택과 순환 신경망 상태를 초기화하고 생성할 문자 수를 설정\n","\n","* 시작 문자열과 순환 신경망 상태를 사용하여 다음 문자를 예측\n","\n","* 다음, `tf.random.categorical`을 사용하여 output logit값을 바탕으로 랜덤 샘플링\n","\n","* 이 예측된 문자를 모델의 다음 입력으로 활용\n","\n","* 하나의 token을 계속 추가하는 방식으로 autoregressive 하게 샘플링하는 방법을 사용함\n","\n","* 이를 통해, 길이가 긴 text를 생성하는 것도 가능함\n","\n","![텍스트를 생성하기 위해 모델의 출력이 입력으로 피드백](https://tensorflow.org/tutorials/text/images/text_generation_sampling.png)\n","\n","- 생성된 텍스트를 보면 모델이 언제 대문자로 나타나고, 절을 만들고 셰익스피어와 유사한 어휘를 가져오는지 알 수 있음\n","- 그러나 훈련 Epoch이 적은 관계로 논리적인 문장을 형성하는 것은 훈련되지 않음"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"WvuwZBX5Ogfd","colab":{}},"source":["def generate_text(model, start_string, temperature=1.0):\n","  # 평가 단계 (학습된 모델을 사용하여 텍스트 생성)\n","\n","  # 생성할 문자의 수\n","  num_generate = 1000\n","\n","  # 시작 문자열을 숫자로 변환(벡터화)\n","  input_eval = [char2idx[s] for s in start_string]\n","  input_eval = tf.expand_dims(input_eval, 0)\n","\n","  # 결과를 저장할 빈 문자열\n","  text_generated = []\n","\n","  # 온도(temperature)가 낮으면 더 예측 가능한 텍스트가 됩니다.\n","  # 온도가 높으면 더 의외의 텍스트가 됩니다.\n","  # 최적의 세팅을 찾기 위한 실험\n","\n","  # 여기에서 배치 크기 == 1\n","  model.reset_states()\n","  for i in range(num_generate):\n","      predictions = model(input_eval)\n","      # 배치 차원 제거\n","      predictions = tf.squeeze(predictions, 0)\n","\n","      # 범주형 분포를 사용하여 모델에서 리턴한 단어 샘플링 (logit 값에 따라 샘플링)\n","      predictions = predictions / temperature\n","      # sample된 token들 중 마지막 token만 선택\n","      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n","\n","      # 예측된 단어를 다음 입력으로 모델에 전달\n","      # 이전 은닉 상태와 함께\n","      input_eval = tf.expand_dims([predicted_id], 0)\n","\n","      text_generated.append(idx2char[predicted_id])\n","\n","  return (start_string + ''.join(text_generated))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ktovv0RFhrkn","colab":{}},"source":["print(generate_text(model, start_string=\"ROMEO: \", temperature=1.0))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"AM2Uma_-yVIq"},"source":["- 결과를 개선하는 가장 쉬운 방법은 더 오래 훈련하는 것(ex: `EPOCHS = 30`).\n","- 이외에 시도해볼 수 있는 것들\n","  - 다른 시작 문자열\n","  - RNN 모델 네트워크 구조 수정\n","  - temperature 조정 \n","    - 값이 작으면, 예측된 logit 값이 큰 token이 주로 샘플링 \n","    - 값이 크면, 예측된 logit 값이 작은 token도 샘플링 될 확률이 높아짐"]},{"cell_type":"markdown","metadata":{"id":"TSLCDUUWfygO","colab_type":"text"},"source":["- 다음 문제 실습을 위해, 학습된 모델 삭제"]},{"cell_type":"code","metadata":{"id":"cx15KNsPfyGj","colab_type":"code","colab":{}},"source":["del model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b-TICyU6hxeq","colab_type":"text"},"source":["## RNN 실습\n","데이터셋은 그대로 이용하고 나머지 부분을 직접 구현해보기"]},{"cell_type":"markdown","metadata":{"id":"DY38VG8OeSYt","colab_type":"text"},"source":["### 문제 1: RNN 모델 만들기\n","- 조건: 딥러닝 모델의 각 layer는 1번부터 5번까지 순서대로 구성\n","1. embedding layer, input_dim: vocab size, output_dim: 256\n","2. LSTM layer, hidden units 512\n","3. layer normalization\n","4. dense layer, hidden node: 256, activation ReLU\n","5. dense layer, hidden node: vocab size\n","- 딥러닝 모델을 만드는 두가지 방식(tf.keras.Model, tf.keras.models.Sequential) 중 선택하여 만들기"]},{"cell_type":"code","metadata":{"id":"X4HfzRHoUi_N","colab_type":"code","colab":{}},"source":["# 여기에 문제 1에 대한 코드를 작성하면 됩니다."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RhOW5zlV6kY1","colab_type":"code","colab":{}},"source":["model = tf.keras.Sequential([\n","    tf.keras.layers.Embedding(input_dim=len(vocab), output_dim=256),\n","    tf.keras.layers.LSTM(512, return_sequences=True),\n","    tf.keras.layers.LayerNormalization(),\n","    tf.keras.layers.Dense(256, activation='relu'),\n","    tf.keras.layers.Dense(vocab_size)\n","])\n","\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZVOIj2jKy77Y","colab_type":"text"},"source":["###옵티마이저(optimizer), 손실 함수, 평가 metric 선택\n","- 기존과 동일함"]},{"cell_type":"code","metadata":{"id":"0KlWlN1wZTfo","colab_type":"code","colab":{}},"source":["model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","              optimizer=tf.keras.optimizers.Adam(),\n","              metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rzJQ4zIRoCD4","colab_type":"text"},"source":["### 모델 학습 및 평가\n","- 체크포인트를 저장하는 checkpoint_dir 및 전체 epoch 수를 제외하고, 기존과 동일함"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"67vSD-KS8rOc","colab":{}},"source":["# 체크포인트가 저장될 디렉토리\n","checkpoint_dir = './checkpoints'\n","\n","# 체크포인트 파일 이름\n","checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n","\n","# model.fit 메소드에 함께 이용할 callback \n","# 'val_loss'를 monitor하면서, 가장 val_loss가 작을때만 모델을 저장함 (save_best_only=True)\n","checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_prefix, \n","    save_weights_only=True, \n","    monitor='val_loss',\n","    save_best_only=True)\n","\n","# 학습 시간을 위해 10 epoch만 학습\n","history = model.fit(train_dataset, epochs=10,\n","                    validation_data=valid_dataset,\n","                    callbacks=[checkpoint_callback])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e3mWxlmMpZOp","colab_type":"text"},"source":["### 문제 2: 학습된 모델 활용"]},{"cell_type":"markdown","metadata":{"id":"GVcHmG8wu8Q3","colab_type":"text"},"source":["위에서 정의된 `generate_text` 함수를 이용하여 텍스트를 생성해보기\n","- 조건1: 학습된 모델중에서 validation loss를 기준으로 best model 이용하기\n","- 조건2: \"JULIET: \"을 `start_string`으로 하고, `temperature`를 0.2, 1.0, 100.0으로 바꾸어 가면서 결과를 출력하고, 출력 결과를 비교하기\n","- 조건3: temperature는 1.0으로 고정하고, `start_string`을 자유롭게 1가지 넣어보기"]},{"cell_type":"code","metadata":{"id":"YZcA_y8iu7mv","colab_type":"code","colab":{}},"source":["# 여기에 문제 2에 대한 코드를 작성하면 됩니다."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Efy4BFeaCOgc","colab_type":"code","colab":{}},"source":["print(tf.train.latest_checkpoint(checkpoint_dir))\n","model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pGPnmt-0ADED","colab_type":"code","colab":{}},"source":["print(generate_text(model, start_string=\"JULIET: \", temperature=0.2))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cV2-5DSaCV4x","colab_type":"code","colab":{}},"source":["print(generate_text(model, start_string=\"JULIET: \", temperature=1.0))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"96eHBVSNCWLm","colab_type":"code","colab":{}},"source":["print(generate_text(model, start_string=\"JULIET: \", temperature=100.0))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fPQ9c3QNDRHW","colab_type":"code","colab":{}},"source":["print(generate_text(model, start_string=\"I'm free now\", temperature=1.0))"],"execution_count":null,"outputs":[]}]}